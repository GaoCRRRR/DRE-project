{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "import torch\n",
    "from torch import nn\n",
    "from model import DRE_model, new_DREmodel\n",
    "from torch.utils.data import Dataset\n",
    "import json\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#加载模型\n",
    "#model=torch.load('models/baseline/12.pth')\n",
    "model=torch.load('models/method/21.pth').cuda()\n",
    "testpath=\"./data/test.json\"\n",
    "devpath=\"./data/dev.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetOutput(path,model):\n",
    "    tokenizer=BertTokenizer.from_pretrained('bert-base-uncased') \n",
    "    InputIds=[]\n",
    "    AttentionMasks=[]\n",
    "    max_seq_len=256\n",
    "    with open(path, encoding='utf8') as fp:\n",
    "        data = json.load(fp)\n",
    "        for item in data:\n",
    "            texts=[]\n",
    "            text = \"\"      \n",
    "            for sentence in item[0]:\n",
    "                text += sentence\n",
    "                text += \" \"\n",
    "                text = text.replace('\"', \"'\")\n",
    "                texts.append(text)\n",
    "            for d in item[1]:\n",
    "                for text in texts:\n",
    "                    newtext = \"[CLS] \" + text + \" [SEP] \"\n",
    "                    if d['x'] == \"Speaker 1\":                       #x为subject，y为object\n",
    "                        newtext = text.replace(\"Speaker 1\", \"X\")    #[S1]:X    [S2]:Y\n",
    "                        newtext += \" X\"\n",
    "                    elif d['x'] == \"Speaker 2\":\n",
    "                        newtext = text.replace(\"Speaker 2\", \"Y\")\n",
    "                        newtext += \" Y\"\n",
    "                    else:\n",
    "                        newtext = newtext + \" \" + d['x']\n",
    "                    newtext += \" [SEP]\"\n",
    "                    if d['y'] == \"Speaker 1\":\n",
    "                        text = text.replace(\"Speaker 1\", \"X\")\n",
    "                        newtext += \" X\"\n",
    "                    elif d['y'] == \"Speaker 2\":\n",
    "                        text = text.replace(\"Speaker 2\", \"Y\")\n",
    "                        newtext += \" Y\"\n",
    "                    else:\n",
    "                        newtext = newtext + \" \" + d['y']\n",
    "#                     newtext += d['x']\n",
    "#                     newtext += \" [SEP]\"\n",
    "#                     newtext += d['y']                    \n",
    "#                     newtext += \" [SEP]\"\n",
    "                    text_dict = tokenizer.encode_plus(newtext, return_attention_mask=True)  # 分词\n",
    "                    l=len(text_dict['input_ids'])\n",
    "                    if(l>max_seq_len):\n",
    "                        InputIds.append(torch.tensor(text_dict['input_ids'])[-max_seq_len-1:-1])\n",
    "                        AttentionMasks.append(torch.tensor(text_dict['attention_mask'])[-max_seq_len-1:-1])\n",
    "                    else:\n",
    "                        pad=nn.ZeroPad2d(padding=(0,max_seq_len-1,0,0))\n",
    "                        InputIds.append(pad(torch.tensor(text_dict['input_ids']).unsqueeze(0)).squeeze(0))\n",
    "                        AttentionMasks.append(pad(torch.tensor(text_dict['attention_mask']).unsqueeze(0)).squeeze(0))\n",
    "    Outputs=[]\n",
    "    for i in range(len(InputIds)):\n",
    "        if(i%1000==0):\n",
    "            print(i)\n",
    "        input_ids=np.array(InputIds[i],dtype=float)\n",
    "        input_ids=input_ids[np.newaxis, :]\n",
    "        input_ids=torch.LongTensor(input_ids).cuda()\n",
    "        attention_masks=np.array(AttentionMasks[i],dtype=float)\n",
    "        attention_masks=attention_masks[np.newaxis, :]\n",
    "        attention_masks=torch.LongTensor(attention_masks).cuda()\n",
    "        with torch.no_grad():\n",
    "            output=model( input_ids,attention_masks )\n",
    "        Outputs.append(output.cpu().squeeze(0).detach().numpy())\n",
    "    #torch.cuda.empty_cache()\n",
    "    return Outputs\n",
    "   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReadData(path):\n",
    "    with open(path, \"r\", encoding='utf8') as f:\n",
    "        data = json.load(f)\n",
    "    for i in range(len(data)):\n",
    "        for j in range(len(data[i][1])):\n",
    "            for k in range(len(data[i][1][j][\"rid\"])):\n",
    "                data[i][1][j][\"rid\"][k] -= 1                    #第37是unawnserable\n",
    "    \n",
    "    return data\n",
    "\n",
    "def Vec2Label(PredVec, Threshold=0.5):\n",
    "    '''\n",
    "    用两个阈值生成标签\n",
    "    for i in range(len(result)):\n",
    "        r = []\n",
    "        maxl, maxj = -1, -1\n",
    "        for j in range(len(result[i])):\n",
    "            if result[i][j] > T1:\n",
    "                r += [j]\n",
    "            if result[i][j] > maxl:\n",
    "                maxl = result[i][j]\n",
    "                maxj = j\n",
    "        if len(r) == 0:\n",
    "            if maxl <= T2:\n",
    "                r = [36]\n",
    "            else:\n",
    "                r += [maxj]\n",
    "        result[i] = r\n",
    "    '''\n",
    "    ##用1个阈值生成标签\n",
    "    for i in range(len(PredVec)):\n",
    "        r = []\n",
    "        for j in range(len(PredVec[i])):\n",
    "            if PredVec[i][j] > Threshold:\n",
    "                r += [j]\n",
    "        if len(r) == 0:\n",
    "                r = [36]\n",
    "        PredVec[i] = r\n",
    "    return PredVec\n",
    "\n",
    "#PredVec是对应data预测的出的标签向量矩阵。每个元素是一个37维的向量：\n",
    "# 共 m(一段对话的轮数，data[i][0]的长度)*s(一段对话的实体对个数)*n(数据集对话个数)\n",
    "def F1_c(PredVec,Data):\n",
    "    index=0   #索引\n",
    "    PredLabel=Vec2Label(PredVec, Threshold= 0.5)\n",
    "    #print(len(PredLabel))\n",
    "    precisions = []  #每一对实体的P_c\n",
    "    recalls = []     #每一对实体的R_c\n",
    "    for i in range(len(Data)):   #遍历数据集\n",
    "        for j in range(len(Data[i][1])):   #遍历一段对话下的标注数据\n",
    "            correct_sys, all_sys = 0, 0\n",
    "            correct_gt = 0          \n",
    "            x = Data[i][1][j][\"x\"].lower().strip()   #头实体\n",
    "            y = Data[i][1][j][\"y\"].lower().strip()   #尾实体\n",
    "            t = {}                                   #建立从关系id到触发词的映射\n",
    "            for k in range(len(Data[i][1][j][\"rid\"])):\n",
    "                if(Data[i][1][j][\"rid\"][k] != 36):\n",
    "                    t[Data[i][1][j][\"rid\"][k]] = Data[i][1][j][\"t\"][k].lower().strip()\n",
    "\n",
    "            l = set(Data[i][1][j][\"rid\"]) - set([36])   #这段标注的关系种类的集合\n",
    "\n",
    "            ex, ey = False, False\n",
    "            et = {}                  #标注标签与预测标签的差集\n",
    "            for r in range(36):\n",
    "                et[r] = r not in l\n",
    "\n",
    "            for k in range(len(Data[i][0])):  #前k轮对话预测的关系种类集合\n",
    "                o = set(PredLabel[index]) - set([36])        #O\n",
    "                e = set()                                    #E\n",
    "                if x in Data[i][0][k].lower():\n",
    "                    ex = True\n",
    "                if y in Data[i][0][k].lower():\n",
    "                    ey = True\n",
    "                if k == len(Data[i][0])-1:\n",
    "                    ex = ey = True\n",
    "                    for r in range(36):\n",
    "                        et[r] = True\n",
    "                for r in range(36):\n",
    "                    if(r in t):\n",
    "                        if(t[r] != \"\" and t[r] in Data[i][0][k].lower()):   #若存在关系r的标记触发词\n",
    "                            et[r] = True\n",
    "                    if(ex and ey and et[r]):\n",
    "                        e.add(r)\n",
    "                correct_sys += len(o & l & e)\n",
    "                all_sys += len(o & e)\n",
    "                correct_gt += len(l & e)\n",
    "                index += 1\n",
    "            \n",
    "            precisions += [correct_sys/all_sys if all_sys != 0 else 1] \n",
    "            recalls += [correct_sys/correct_gt if correct_gt != 0 else 0]\n",
    "    print(index)\n",
    "    precision = sum(precisions) / len(precisions)\n",
    "    recall = sum(recalls) / len(recalls)\n",
    "    f_1 = 2*precision*recall/(precision+recall) if precision+recall != 0 else 0\n",
    "\n",
    "    return precision, recall, f_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DevPred=GetOutput(devpath,model)\n",
    "DevData=ReadData(devpath)\n",
    "P, R, F1 = F1_c(DevPred,DevData)\n",
    "print(\"Precision_c = \", P * 100, \"%\")\n",
    "print(\"Recall_c = \", R * 100, \"%\")\n",
    "print(\"F1_c = \", F1 * 100, \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TestPred=GetOutput(testpath,model)\n",
    "TestData=ReadData(testpath)\n",
    "P, R, F1 = F1_c(TestPred,TestData)\n",
    "print(\"Precision_c = \", P * 100, \"%\")\n",
    "print(\"Recall_c = \", R * 100, \"%\")\n",
    "print(\"F1_c = \", F1 * 100, \"%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
